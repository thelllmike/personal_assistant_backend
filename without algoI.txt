from fastapi import FastAPI
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForQuestionAnswering
import torch

# Initialize the FastAPI app
app = FastAPI()

# Load the tokenizer and model from the Hugging Face Transformers library
tokenizer = AutoTokenizer.from_pretrained("bert-large-uncased-whole-word-masking-finetuned-squad")
model = AutoModelForQuestionAnswering.from_pretrained("bert-large-uncased-whole-word-masking-finetuned-squad")

class QuestionContext(BaseModel):
    question: str
    context: str

@app.get("/")
async def read_root():
    """A simple root path to ensure the API is working"""
    return {"Hello": "World"}

@app.post("/ask")
async def answer_question(item: QuestionContext):
    """POST endpoint to ask a question with provided context and get an answer"""
    inputs = tokenizer.encode_plus(item.question, item.context, add_special_tokens=True, return_tensors="pt")
    input_ids = inputs["input_ids"].tolist()[0]

    # Get model output
    outputs = model(**inputs)
    answer_start_scores = outputs.start_logits
    answer_end_scores = outputs.end_logits

    # Find the position of the start and end of the answer in the context
    answer_start = torch.argmax(answer_start_scores)
    answer_end = torch.argmax(answer_end_scores) + 1

    # Convert tokens to the answer string
    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))
    return {"answer": answer}

